<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>NanoBot FAQ</title>
    <link rel="stylesheet" href="/css/main.css">
</head>

<body>
    <div class="nav">
        <button class="selected">Information</button>
        <button onclick="location.href='/'">Return to pyxfluff.dev</button>
    </div>
    <div class="tab-content">
        <h1>What is it?</h1>
        <p>
            NanoBot is a crawler for a search engine in development. It's intention is to crawl furry related websites to develop a furry search engine.
        </p>
        <h2>Why so many requests?</h2>
        <p>
            The bot is designed to recursively follow every link it discovers. It respects robots.txt files but if you would like to reach out to us directly to request blockage or limits, please
            contact me on Discord: @pyxfloof
        </p>
        <h2>How did this find my site?</h2>
        <p>As previously stated, it recursively visits every site it discovers. We probably found the link from another site.</p>
        <h2>Is this for AI training?</h2>
        <p>
            <strong>No.</strong> We are not using any data for AI or other content training. The data we get from your website is just ours and does not belong to any AI company and will not be used
            to train an AI.
        </p>
        <h2>Is my art being stolen?</h2>
        <p>
            <strong>No.</strong> We strongly disagree with stealing art (especially for AI). If we end up using images collected by the bot, we will give sources, owner information, and allow
            you to opt-out easily. However, the main purpose of the crawler is to discover blogs, portfolios, and other websites from talented individuals which are not being seen on existing
            platforms like Google.
        </p>
        <h2>How can I identify it?</h2>
        <p>
            Our user agent is
            <code>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137 Safari/537.36 (compatible; NanoBot/1.0; +https://pyxfluff.dev/nanobot)</code>. The Chrome version is
            dynamic and we will sometimes use Firefox when required.
        </p>
        <h2>Can I block it?</h2>
        <p>
            To stop us from crawling a page, simply add <code>Nanobot</code> to your robots.txt file. We also respect <code>Crawl-delay</code>. However, as previously stated, we would prefer you
            contact s directly via Discord to attempt to resolve any issues beforehand.
        </p>
        <h2>What is it written in? Is it open source?</h2>
        <p>
            The crawler is not currently open source, but it's written in Python using Playwright with a mix of httpx and urllib. The backend database for keeping track of visited pages and images is
            MongoDB 8.
        </p>
        <h2>Shortcomings</h2>
        <p>
            While we respect robots.txt files, I've observed it may have trouble following path wildcards due to an issue with the urllib parse function. This will be fixed soon.
            <br><br>
            The bot also does not prioritize URLs or sort them by anything, it simply keeps them in an in-memory list and will attempt to visit as many as possible given it is not blocked by our local
            blocklist or the robots.txt file. After our initial crawl (which is still currently underway) we will likely implement a system where pages can be crawled based on internal priority.
        </p>
        <h2>Nanobot is not following my Crawl-Delay!</h2>
        <p>
            Short answer: Yes it is, probably.
            <br>
            Long answer: Crawl-Delay is read at the time of site discovery. It is respected on a thread level for non-static content. So, for example, if your site
            is being visited on one thread and gets discovered on another, they are unable to communicate with eachother, and they both will become out of sync and do not do anything but respect their
            own clocks. Eventually, we will improve this behavior. The bot is still in testing now, so we need to iron our quirks like this.
        </p>
        <h2>How many threads is the indexer running on?</h2>
        <p>
            We don't allocate any specific number of threads to crawl and we are constantly tweaking how much crawling and nudenet scanning is being done at once.
        </p>
        <h2>What will stop my public site or social media profile from being indexed?</h2>
        <p>The following things can result in being delisted:
        <ul>
            <li>Non-2xx status code (404, 500, etc)</li>
            <li>Cloudflare block/captcha ("Just a moment.." in your title block; this is why Wikifur is incomplete)</li>
            <li>Private/delisted page on social media</li>
            <li>Robots.txt block (e621 disallows visiting profiles)</li>
            <li>Manual block (we block some sites if they are leading the crawler astray (wikipedia); mostly news & other mainstream websites with lots of pages)</li>
            <li>Abusive/illegal content</li>
        </ul>

        We also have not built a <i>complete</i> index of popular websites like FA yet, so your social & personal pages may not yet be indexed. In the future, we will allow for forcefully requesting
        an index to bypass any restrictions that may be in place to prevent it from crawling random sites.

        </p>

        <h2>Why is it crawling in "waves"?</h2>
        <p>Sometimes, if the bot discovers a lot of links on one page, it will take a break from your site to try and clear those. We also sometimes pause indexing for compute-intensive database jobs
            or network issues.</p>
            
        <h2>Does the bot detect change?</h2>
        <p>Not at the moment. As of our initial run, we are just taking in as many pages as possible and will set up change detection at a later date so we can stay up-to-date on removed art or banned
            users.</p>
    </div>
</body>

</html>
